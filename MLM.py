import pandas as pd
import torch
import numpy as np
import os
from torch.nn.parallel import DistributedDataParallel
from torch.optim import AdamW
from datasets import DatasetDict, Dataset
from transformers import AutoTokenizer, DataCollatorForLanguageModeling,Trainer,TrainingArguments, AutoModel,AutoModelForMaskedLM
from torch.utils.data.distributed import DistributedSampler
from tqdm import tqdm
import argparse
import time
import pprint
from torch.utils.data import DataLoader, TensorDataset
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

def run_training(args, model, train_data,colator):
    print(f"Starting main loop")

    training_args = TrainingArguments(
        report_to='tensorboard',
        output_dir=args.save_dir,
        overwrite_output_dir=False,

        do_train=True,
        save_strategy='epoch',

        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size_per_replica,
        gradient_accumulation_steps=args.grad_acc_steps,

        learning_rate=args.lr,
        weight_decay=0.05,
        warmup_steps=args.lr_warmup_steps,

        logging_dir=args.save_dir,
        logging_first_step=True,
        logging_steps=args.log_freq,
        save_total_limit=1,

        dataloader_drop_last=True,
        dataloader_num_workers=4,

        local_rank=args.local_rank,

    )
    print(train_data['train'])
    trainer = Trainer(
        model=model,
        data_collator=colator,
        args=training_args,
        train_dataset=train_data['train'],
       
      
    )

    trainer.train()
    
    if args.local_rank in [0, -1]:
        final_checkpoint_dir = os.path.join(args.save_dir, "final_checkpoint")
        model.save_pretrained(final_checkpoint_dir)
        print(f'  ==> Finish training and save to {final_checkpoint_dir}')
def tokenize_function(examples):
    token = AutoTokenizer.from_pretrained('microsoft/codebert-base')
    inputs = token(
            examples['0'],
            None,
            truncation=True,
            padding = 'max_length',
            max_length =512,
            add_special_tokens=True,
            return_token_type_ids=True,
            return_tensors="pt"
        )
    # inputs["labels"] = inputs["input_ids"]
    return inputs
        
def load_tokenize_data(args):
    df = pd.read_csv("/home/bkcs/HDD/Thep/func.csv")
    data_dict = {
        'train': df[:]
    }

    train_dataset = Dataset.from_pandas(data_dict['train'])
    raw_datasets = DatasetDict(
        {
            "train": train_dataset,
        }
    )
    
    tokenized_datasets = raw_datasets.map(
        tokenize_function, batched=True, remove_columns=['Unnamed: 0', '0']
    )
    tokenizer = AutoTokenizer.from_pretrained(args.load)
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
    return tokenized_datasets,data_collator


def main(args):
    argsdict = vars(args)
    print(pprint.pformat(argsdict))

    # Save command to file
    with open(os.path.join(args.save_dir, "command.txt"), 'w') as f:
        f.write(pprint.pformat(argsdict))

    # Load and tokenize data using the tokenizer from `args.load`. If the data is already cached, load it from there.
    # You can customize this function to load your own data for any Seq2Seq LM tasks.
    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)  
    train_data , collator= load_tokenize_data(args)

  
    # Load model from `args.load`
    model = AutoModelForMaskedLM.from_pretrained(args.load)
    print(f"  ==> Loaded model from {args.load}, model size {model.num_parameters()}")

    run_training(args, model, train_data,collator)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="CodeT5+ finetuning on Seq2Seq LM task")
    parser.add_argument('--data-num', default=-1, type=int)
    parser.add_argument('--max-source-len', default=512, type=int)
    parser.add_argument('--max-target-len', default=512, type=int)
    parser.add_argument('--cache-data', default='/home/bkcs/HDD/Thep/test1', type=str)
    parser.add_argument('--load', default='microsoft/codebert-base', type=str)

    # Training
    parser.add_argument('--epochs', default=1, type=int)
    parser.add_argument('--lr', default=5e-5, type=float)
    parser.add_argument('--lr-warmup-steps', default=100, type=int)
    parser.add_argument('--batch-size-per-replica', default=4, type=int)
    parser.add_argument('--grad-acc-steps', default=1, type=int)
    parser.add_argument('--local_rank', default=-1, type=int)

    parser.add_argument('--fp16', default=False, action='store_true')

    # Logging and stuff
    parser.add_argument('--save-dir', default="/home/bkcs/HDD/Thep/MLM", type=str)
    parser.add_argument('--log-freq', default=10, type=int)
    parser.add_argument('--save-freq', default=500, type=int)

    args = parser.parse_args()

    os.makedirs(args.save_dir, exist_ok=True)

    main(args)
