import torch
import argparse
import deepspeed
import os
import time
import torch.nn as nn
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader,Dataset
from sklearn.metrics import classification_report,accuracy_score,hamming_loss
from transformers import AutoTokenizer, AutoModel
deepspeed.init_distributed()
import re

def remove_comments(code, lang):
    if lang.lower() in ['c', 'c++', 'cpp']:
        # Loại bỏ comment từ mã C/C++
        return re.sub(r'//.*|/\*(.|\n)*?\*/', '', code)
    elif lang.lower() == 'python':
        # Loại bỏ comment từ mã Python
        return re.sub(r'#.*', '', code)
    elif lang.lower() == 'php':
        # Loại bỏ comment từ mã PHP
        return re.sub(r'//.*|/\*.*?\*/', '', code)
    elif lang.lower() == 'javascript':
        # Loại bỏ comment từ mã JavaScript
        return re.sub(r'//.*|/\*(.|\n)*?\*/', '', code)
    elif lang.lower() in ['html', 'markdown']:
        # Loại bỏ comment từ mã HTML hoặc Markdown
        return re.sub(r'<!--(.|\n)*?-->|<!--.*?-->', '', code)
    elif lang.lower() in ['shell', 'bash']:
        return re.sub(r'#.*', '', code)
    elif lang.lower() == 'typescript':
        return re.sub(r'//.*|/\*(.|\n)*?\*/', '', code)
    elif lang.lower() == 'ruby':
        return re.sub(r'#.*', '', code)
    else:
        return code
def add_argument():
    parser = argparse.ArgumentParser(description='TRAIN')
    parser.add_argument('--with_cuda',
                        default=False,
                        action='store_true',
                        help='use CPU in case there\'s no GPU support')

    parser.add_argument('-e',
                        '--epochs',
                        default=10,
                        type=int,
                        help='number of total epochs (default: 30)')
    parser.add_argument('--local_rank',
                        type=int,
                        default=-1,
                        help='local rank passed from distributed launcher')

    parser.add_argument('--log-interval',
                        type=int,
                        default=2000,
                        help="output logging information at a given interval")
    parser = deepspeed.add_config_arguments(parser)
    args = parser.parse_args()
    return args


args = add_argument()

class Branch(nn.Module):
  def __init__(self, input_size, hidden_size, dropout, num_outputs):
    super(Branch, self).__init__()

    self.dense1 = nn.Linear(input_size, hidden_size)
    self.batchnorm1 = nn.BatchNorm1d(hidden_size)
    self.dropout = nn.Dropout(p=dropout)
    self.dense2 = nn.Linear(hidden_size, num_outputs)

  def forward(self, x):
    out_dense1 = self.dense1(x)
    out_batchnorm1 = self.batchnorm1(out_dense1)
    out_dropout = self.dropout(out_batchnorm1)
    out_dense2 = self.dense2(out_dropout)

    return out_dense2


class BaseModel(nn.Module):
    def __init__(self, original_model, num_classes,  is_multibranches=False):
        super(BaseModel, self).__init__()
        self.num_classes = num_classes
        self.original_model = original_model
        self.is_multibranches = is_multibranches
        branch_feature_size = 768
        if self.is_multibranches:
            self.branches = nn.ModuleList([Branch(branch_feature_size, 768, 0.1, 1) for _ in range(num_classes)])
        else:
            self.branch = Branch(branch_feature_size, 768, 0.2, num_classes)
        
        self.activation = nn.Sigmoid()

    def forward(self,inputs):
        out_bert = self.original_model(input_ids=inputs['input_ids'].squeeze().view(-1,512),attention_mask=inputs['attention_mask'].squeeze().view(-1,512))
        # pooler_out = torch.mean(out_bert.last_hidden_state,dim=1)
        pooler_out =out_bert.last_hidden_state[:,0,:]
        if self.is_multibranches:
            output_branches = [branch(pooler_out) for branch in self.branches]
            out_branch = torch.cat(output_branches, dim=1)
        else:
            out_branch = self.branch(pooler_out)
        
        outputs = self.activation(out_branch)

        return outputs
    
    
class OpcodeData(Dataset):
    def __init__(self, X, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.X = X
        
        self.max_len = max_len
        self.target = X.loc[:, [ 'access-control', 'arithmetic', 'reentrancy',  'unchecked-calls']]
   
    def __len__(self):
        return len(self.X)
    def __getitem__(self, index):
        values = self.X.iloc[index]['source_code']
        row = self.target.iloc[index]
        tensor_row = torch.tensor(row)
        inputs = self.tokenizer(
            values,
            None,
            truncation=True,
            padding = 'max_length',
            max_length = self.max_len,
            add_special_tokens=True,
            return_token_type_ids=True,
            return_tensors="pt"
        )

        return {
            'inputs': inputs,
            'targets': torch.tensor(tensor_row, dtype=torch.float32)
        }
max_length = 512
data_folder='/home/bkcs/HDD/Thep/'

data = pd.read_feather(data_folder+'new_data.feather')
X_train = data.loc[data['dataset'] == 'train']
X_test = data.loc[data['dataset'] == 'test']                                                                    
X_val = data.loc[data['dataset'] == 'val']                                                                                                                                              




token = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')

training_set = OpcodeData(X_train, token, max_length)
validating_set = OpcodeData(X_val, token, max_length)
testing_set = OpcodeData(X_test, token, max_length)                                                                                             


training_loader = DataLoader(training_set, batch_size=batch_size,shuffle=True,drop_last=True)
validating_loader = DataLoader(validating_set, batch_size=batch_size,shuffle=True,drop_last=True)
testing_loader = DataLoader(testing_set, batch_size=batch_size,shuffle=True,drop_last=True)


model=  AutoModel.from_pretrained('google-bert/bert-base-uncased')
for param in model.encoder.parameters():                                                                                                                                                                                                            
    param.requires_grad = False

net = BaseModel(model,4)
parameters = filter(lambda p: p.requires_grad, net.parameters())

model_engine, optimizer,__, __ = deepspeed.initialize(
    args=args, model=net, model_parameters=parameters)

fp16 = model_engine.fp16_enabled()
print(f'fp16={fp16}')                                                                                                                                                                                                                                                                                                                                                                                                       




criterion = nn.BCELoss()




def evaluate_steps(model,validate_loader):
    print("Evaluating...")
    model.eval()

    total_loss = 0
    total_preds = torch.tensor([])  # Initialize as empty tensor
    total_labels = torch.tensor([])


    for  i, data  in enumerate(validate_loader):
        # push the batch to gpu
        inputs, labels = data['inputs'].to(model.local_rank), data['targets'].to(
            model.local_rank)


        with torch.no_grad():
            if fp16:
                inputs = inputs.half()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
     
            total_loss += loss.item()
           
            if i == 0:  # Initialize tensors only in the first iteration
                total_preds = torch.empty(0, *outputs.shape[1:], device=outputs.device)
                total_labels = torch.empty(0, *labels.shape[1:], device=labels.device)

            # Concatenate predictions and labels along the first dimension (batch_size)
            total_preds = torch.cat((total_preds, outputs), dim=0)
            total_labels = torch.cat((total_labels, labels), dim=0)
        


        
    TP = ((total_preds > 0.5) & (total_labels > 0.5)).sum(1).float() 
    TN = ((total_preds <= 0.5) & (total_labels <= 0.5)).sum(1).float() 
    FP = ((total_preds > 0.5) & (total_labels <= 0.5)).sum(1).float() 
    FN = ((total_preds <= 0.5) & (total_labels > 0.5)).sum(1).float() 

    acc = torch.mean(TP/(TP+FN+FP))

    return acc , total_loss / len(validating_loader)
            
best_val_metric = 0
path ='/home/bkcs/HDD/Thep/new-result/BERT/BERT_firstoken'

loss_file_path = os.path.join(path, "loss.txt")
loss_file_path_val = os.path.join(path, "loss_val.txt")
start_time = time.time()


for epoch in range(10):  
    running_loss = 0.0
    model_engine.train() 
    for i, data in enumerate(training_loader):
        
        inputs, labels = data['inputs'].to(model_engine.local_rank), data['targets'].to(
            model_engine.local_rank)
        if fp16:
            inputs = inputs.half()
        outputs = model_engine(inputs)
        loss = criterion(outputs, labels)

        model_engine.backward(loss)
        model_engine.step()
        
        # print statistics
        running_loss += loss.item()

        if i % args.log_interval == (
                args.log_interval -
                1):  # print every log_interval mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / args.log_interval))
            with open(loss_file_path, "a") as loss_file:
                loss_file.write(f"{epoch + 1},{i + 1},{running_loss / args.log_interval}\n")
            running_loss = 0.0
        
            
            

    val_metric,val_loss = evaluate_steps(model_engine, validating_loader)  # Define your evaluate_on_val_set function
    with open(loss_file_path_val, "a") as loss_file_val:
        loss_file_val.write(f"{epoch + 1},{val_loss},{val_metric}\n")

    print(val_metric)
    
    if val_metric > best_val_metric:
        best_val_metric = val_metric
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': model_engine.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
        }, path + 'best_model.pth')
end_time = time.time()
total_time_seconds = end_time - start_time

print('Finished Training')


torch.save({
    'epoch': epoch,
    'model_state_dict': model_engine.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, path + 'finalpath.pth')


class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
sample_runtimes = []
def evaluate_steps_test(model,validate_loader):
    print("Evaluating...")
    model.eval()

    total_loss = 0
    total_preds = torch.tensor([])  # Initialize as empty tensor
    total_labels = torch.tensor([])

    
    
    for  i, data  in enumerate(validate_loader):
        # push the batch to gpu
        inputs, labels = data['inputs'].to(model.local_rank), data['targets'].to(
            model.local_rank)

        
        with torch.no_grad():
            if fp16:
                inputs = inputs.half()
            start_time_k = time.time()
            outputs = model(inputs)
            end_time_k = time.time()
            if i == 0:  # Initialize tensors only in the first iteration
                total_preds = torch.empty(0, *outputs.shape[1:], device=outputs.device)
                total_labels = torch.empty(0, *labels.shape[1:], device=labels.device)
            total_preds = torch.cat((total_preds, (outputs>0.5)), dim=0)
            total_labels = torch.cat((total_labels, labels), dim=0)
        sample_runtimes.append(end_time_k - start_time_k)
    average_runtime = np.mean(sample_runtimes)/batch_size

    TP = ((total_preds > 0.5) & (total_labels > 0.5)).sum(1).float() 
    TN = ((total_preds <= 0.5) & (total_labels <= 0.5)).sum(1).float() 
    FP = ((total_preds > 0.5) & (total_labels <= 0.5)).sum(1).float() 
    FN = ((total_preds <= 0.5) & (total_labels > 0.5)).sum(1).float() 

    labels = total_labels.cpu().numpy()
    preds = total_preds.cpu().numpy()

    label_names = [ 'access-control', 'arithmetic', 'reentrancy',  'unchecked-calls']
    out = classification_report( labels,preds, output_dict=True,target_names=label_names)
    total_support = out['samples avg']['support']
    acc = accuracy_score(labels,preds)
    hm = hamming_loss(labels,preds)

    acc_s =  torch.mean(TP/(TP+FN+FP)).cpu().numpy()

    out['Exact Match Ratio'] = {'precision': acc, 'recall': acc, 'f1-score': acc, 'support': total_support}
    out['Hamming Loss'] = {'precision': hm, 'recall': hm, 'f1-score': hm, 'support': total_support}
    out['Accuracy'] = {'precision': acc_s, 'recall': acc_s, 'f1-score': acc_s, 'support': total_support}
    out['TimeTraing'] = {'precision': total_time_seconds, 'recall': total_time_seconds, 'f1-score': total_time_seconds, 'support': total_time_seconds}
    out['Timepersample'] = {'precision': average_runtime, 'recall': average_runtime, 'f1-score': average_runtime, 'support': average_runtime}
    
    out_df = pd.DataFrame(out).transpose()
    print(out_df)

    out_df.to_csv(path+"result.csv")
    
    return out_df
checkpoint = torch.load( path + 'best_model.pth')
model_engine.load_state_dict(checkpoint['model_state_dict'])
with torch.no_grad():
    evaluate_steps_test(model_engine, testing_loader)

