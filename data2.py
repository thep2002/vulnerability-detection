import subprocess
import re
import os
import torch
import dgl
from tqdm import tqdm
import numpy as np
import pandas as pd
from transformers import AutoTokenizer,AutoModel
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Pool
import threading
vul = []
tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')
model=  AutoModel.from_pretrained('/home/bkcs/HDD/Thep/MLM/final_checkpoint')
device = 'cuda'
model.to(device)
def list_to_string(lst):
    result = ' '.join(map(str, lst)) 
    return result
def extend_matrix_by_one(matrix, fill_value=0):
    current_rows, current_cols = matrix.shape
    # Tạo ma trận mới với kích thước lớn hơn một hàng và một cột
    new_matrix = np.full((current_rows + 1, current_cols + 1), fill_value)
    
    # Sao chép ma trận ban đầu vào ma trận mới
    new_matrix[:current_rows, :current_cols] = matrix
    
    return new_matrix

def reduce_matrix_by_one(matrix):
    current_rows, current_cols = matrix.shape
    # Check if the matrix has more than one row and one column
    if current_rows <= 1 or current_cols <= 1:
        raise ValueError("Matrix must have more than one row and one column to reduce its size.")
    
    # Create a new matrix with one less row and one less column
    new_matrix = matrix[:current_rows-1, :current_cols-1]
    
    return new_matrix

def extract_contracts(input_text):
    input_text = re.sub(r'\b://', ':/', input_text)
    no_line_comments = re.sub(r'(?<!:)//.*', '', input_text)
    no_block_comments = re.sub(r'\/\*[\s\S]*?\*\/', '', no_line_comments)
    no_line_breaks = re.sub(r'\n', ' ', no_block_comments)
    no_extra_spaces = re.sub(r'\s+', ' ', no_line_breaks)
    return no_extra_spaces

def extract_content(text, position):
    con = []
    position = position - 1
    brace_count = 0
    record = True
    check = True
    check1 = False

    while record:
        w = text[position]
        con.append(w)
        if  '{' in w:
            check = False
            brace_count += 1
        elif '}' in w:
            brace_count -= 1
            if brace_count ==0:
                record = False
        if check and ';' in w:
            break
        if ('function' in  w or( (position+1) == len(text) )) and check1:
            break
        check1 = True
        position += 1
        
    return con

def fin(word, string):
    for i,w in enumerate(word):
        if i+1 == len(word):
            break
        if string in w:
            if '(' in w or '(' in word[i+1]:
                return i
    return -1

def rfin(word, string):
    for i in range(len(word) - 1, -1, -1):
        if string in word[i]:
            if '(' in word[i] or '(' in word[i+1]:
                return i
    return -1

def fin_exactly(stringFun,stringCon,lo):
    cons = False
    for i,w in enumerate(lo):
        if stringCon in w and 'contract' == lo[i-1] :
            cons = True
        if stringFun in w and cons and 'function' in lo[i-1]:
            if '(' in w or  '(' in lo[i+1]:
                return i
    return -1

def fin_exactly_s(stringCon,lo):
    cons = False
    for i,w in enumerate(lo):
        if stringCon in w and 'contract' in lo[i-1] :
            cons = True
        elif cons:
            if 'contract' in w:
                return -1
        if 'function()' == w or 'fallback()' == w or 'receive()' == w:
            return i
        if 'function' == w or 'fallback' == w or 'receive' == w:
            if lo[i+1] == '()':
                return i

    return -1


def ex_data(source,label,er,a,lable1):

    cons = []
    func_name = []
    func = []
    counts = []
    path = 'abc.txt'
    with open(path, 'w') as file:
        file.write(source)
    node_script_path = './index.js'
    try:
        result = subprocess.run(['node', node_script_path,path] , capture_output=True, text=True, check=True)
        data = str(result.stdout).split()
    except subprocess.CalledProcessError as e:
        print("Error:", e.stderr)
    # with open(filePath, 'r', encoding='utf-8', errors='replace') as file_l:
    #     input_text = file_l.read()
    if(len(data) == 0):
        er = er + 1 
        print('eer2',er,a)
        return er
    l = 0
    
    for i in range(int(data[0])):
        cons.append(data[i+1])
        counts.append(int(data[int(data[0])+1+i]))
        l = l + int(data[i+int(data[0])+1])
    for i in range(int(data[0])*2+1,len(data)):
        func_name.append(data[i])
    if len(func_name) == 0:
        er = er + 1 
        print('eer2',er,a)
        return er
    adj = np.zeros((1, 1), dtype=int)
    lo = source.split()
    new_list = []
    for count, element in zip(counts, cons):
        new_list.extend([element] * count)
    for i,row in enumerate(func_name):
        pos = fin_exactly(func_name[i],new_list[i],lo)
        if pos == -1:
            er = er + 1 
            print('eer' ,er, a)
            return er 
        fun_ex = extract_content(lo,pos)    
        fun_i = list_to_string(fun_ex)
        func.append(fun_i)
        for j in range(i):
            if i == j:
                if fin(fun_ex,func_name[i]) == rfin(fun_ex,func_name[i]):
                    adj[i][i] = 1
            else:
                if fin(fun_ex,func_name[j]) != -1:
                    adj[i][j] = 1
                    adj[j][i] = 1
        adj = extend_matrix_by_one(adj)  
    num_func = len(func)
    if l != len(func):
        for c in cons:
            position = fin_exactly_s(c,lo)
            if position == -1:
                continue
            else:
                fun_ex = extract_content(lo,position+1)
                fun_i = list_to_string(fun_ex)
                func.append(fun_i)   
                for j,r in enumerate(func_name):
                    
                    if fin(fun_ex,func_name[j]) != -1:
                        adj[num_func][j] = 1
                        adj[j][num_func] = 1
   
                num_func += 1
                adj = extend_matrix_by_one(adj)
    adj = reduce_matrix_by_one(adj)  

    

    inputs = tokenizer(func, padding='max_length', max_length=512, truncation=True, return_tensors='pt')
    inputs = {key: value.to(device) for key, value in inputs.items()}

    try:
        with torch.no_grad():
            outputs = model(**inputs)
    except RuntimeError as e:
        if 'out of memory' in str(e):
            print("Out of memory error encountered. Try reducing the batch size or max length of the input sequences.")
            er= er + 1
            return er
        else:
            raise e

    embeddings = outputs.last_hidden_state[:,0,:]
    src, dst = np.nonzero(adj)
    g = dgl.graph((src, dst),num_nodes=adj.shape[0])
    g.ndata['feat'] = embeddings.to('cpu')
    vul.append(g)
    
    lable1.append(label)
    return er
def process_rows():
    er = 0
    df = pd.read_feather('new_data.feather')
    progress_bar = tqdm(total=df.shape[0], desc='Processing', unit='iteration')
    label = []
    for index, row in df.iloc[0:df.shape[0]].iterrows():
        er = ex_data(row['source_code'],row[ [ 'access-control', 'arithmetic', 'reentrancy',  'unchecked-calls']], er, row['address'],label)
   # er = ex_data(df.iloc[340]['source_code'],df.iloc[340][ [ 'access-control', 'arithmetic', 'reentrancy',  'unchecked-calls']], er, vul, df.iloc[340]['address'])
        progress_bar.update(1)
    batch_graph = dgl.batch(vul)
    df = pd.DataFrame(label, columns=[ 'access-control', 'arithmetic', 'reentrancy',  'unchecked-calls'])
    df.to_csv('label_tune.csv')
    
# Lưu batch graph vào file
    torch.save(batch_graph, 'batch_dgl_graph_tune.pt')
        
# def process_file():

#     chunk_size = df.shape[0] // 3
#     threads = []
#     er = 0
#     for i in range(0, df.shape[0], chunk_size):
#         thread_id = len(threads)
#         t = threading.Thread(target=process_rows, args=(i, min(i + chunk_size, df.shape[0]), progress_bar,df,er,thread_id))
#         threads.append(t)
#         t.start()
process_rows()
# # Wait for all threads to complete
#     for t in threads:
#         t.join()


# if __name__ == "__main__":
#     process_file()