import torch
import argparse
import deepspeed
import os
import torch.nn as nn
from sklearn.metrics import classification_report,accuracy_score,hamming_loss
import pandas as pd
from torch.utils.data import DataLoader,Dataset
from transformers import AutoTokenizer, AutoModel

def process_long_text_sequences_from_dataframe(df, column_name, tokenizer, max_length=512, overlap=0):
    processed_tensors = []
    for _, row in df.iterrows():
        text = row[column_name]
        tokens = tokenizer.encode(text,padding = False, return_tensors="pt", truncation=False)
        if tokens.size(1) > max_length:
            i = 0
            while(True):
                if i == 0:
                    slice = tokens[:,0: max_length - 1]
                    slices = torch.cat([slice, torch.tensor([[tokenizer.sep_token_id]])], dim=1)
                elif (i + max_length - 2) > tokens.size(1):
                    slice = tokens[:,i: tokens.size(1)]
                    slices = torch.cat([ torch.tensor([[tokenizer.cls_token_id]]),slice], dim=1) 
                    break
                else:
                    slice = tokens[:,i+1: i + max_length -1]
                    slices = torch.cat([torch.tensor([[tokenizer.cls_token_id]]), slice, torch.tensor([[tokenizer.sep_token_id]])], dim=1)
                i +=  max_length - 2 - overlap
                processed_tensors.extend(slices)

        else:
            processed_tensors.extend(tokens)
    return processed_tensors
def add_argument():
    parser = argparse.ArgumentParser(description='TRAIN')
    parser.add_argument('--with_cuda',
                        default=False,
                        action='store_true',
                        help='use CPU in case there\'s no GPU support')

    parser.add_argument('-e',
                        '--epochs',
                        default=30,
                        type=int,
                        help='number of total epochs (default: 30)')
    parser.add_argument('--local_rank',
                        type=int,
                        default=-1,
                        help='local rank passed from distributed launcher')

    parser.add_argument('--log-interval',
                        type=int,
                        default=2000,
                        help="output logging information at a given interval")

    parser = deepspeed.add_config_arguments(parser)
    args = parser.parse_args()
    return args




args = add_argument()

class Branch(nn.Module):
  def __init__(self, input_size, hidden_size, dropout, num_outputs):
    super(Branch, self).__init__()

    self.dense1 = nn.Linear(input_size, hidden_size)
    self.batchnorm1 = nn.BatchNorm1d(hidden_size)
    self.dropout = nn.Dropout(p=dropout)
    self.dense2 = nn.Linear(hidden_size, num_outputs)

  def forward(self, x):
    out_dense1 = self.dense1(x)
    out_batchnorm1 = self.batchnorm1(out_dense1)
    out_dropout = self.dropout(out_batchnorm1)
    out_dense2 = self.dense2(out_dropout)

    return out_dense2


class BaseModel(nn.Module):
    def __init__(self, original_model, num_classes,  is_multibranches=False):
        super(BaseModel, self).__init__()
        self.num_classes = num_classes
        self.original_model = original_model
        self.is_multibranches = is_multibranches
        branch_feature_size = 768
        if self.is_multibranches:
            self.branches = nn.ModuleList([Branch(branch_feature_size, 128, 0.1, 2) for _ in range(num_classes)])
        else:
            self.branch = Branch(branch_feature_size, 128, 0.2, num_classes)
        
        self.activation = nn.Sigmoid()

    def forward(self,inputs):
        out_bert = self.original_model(input_ids=inputs['input_ids'].squeeze().view(-1,1024*3),attention_mask=inputs['attention_mask'].squeeze().view(-1,1024*3))
        pooler_out = out_bert.last_hidden_state
        pooler_out = torch.mean(pooler_out, dim=1)
        if self.is_multibranches:
            output_branches = [branch(pooler_out) for branch in self.branches]
            out_branch = torch.cat(output_branches, dim=1)
        else:
            out_branch = self.branch(pooler_out)
        
        outputs = self.activation(out_branch)
        return outputs
    
    
class OpcodeData(Dataset):
    def __init__(self, X, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.X = X
        
        self.max_len = max_len
        self.target = X.loc[:, ['Delegatecall Injection', 'Frozen Ether', 'Outdated Solidity version', 'Reentrancy', 'Timestamp dependence']]
   
    def __len__(self):
        return len(self.X)

    def __getitem__(self, index):
        values = self.X.iloc[index]['code']
        row = self.target.iloc[index]
        tensor_row = torch.tensor(row)
        inputs = self.tokenizer(
            values,
            None,
            truncation=True,
            padding='max_length',
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=True,
            return_tensors="pt"
        )

        return {
            'inputs': inputs,
            'targets': torch.tensor(tensor_row, dtype=torch.float64)
        }
model=  AutoModel.from_pretrained('microsoft/codebert-base')
for param in model.encoder.parameters():
    param.requires_grad = False

net = BaseModel(model.encoder,3)
checkpoint = torch.load('./NCKH_result/best_model.pth')

parameters = filter(lambda p: p.requires_grad, net.parameters())
model_engine, optimizer,__, __ = deepspeed.initialize(
    args=args, model=net, model_parameters=parameters)
net.load_state_dict(checkpoint['model_state_dict'])
fp16 = model_engine.fp16_enabled()
print(f'fp16={fp16}')

data_folder='/home/bkcs/HDD/Thep/'

max_length = 1024*3

X_train = pd.read_feather(data_folder+'train_data.feather')
X_test = pd.read_feather(data_folder+'test_data.feather')
X_val = pd.read_feather(data_folder+'val_data.feather')


token = AutoTokenizer.from_pretrained('Salesforce/codet5p-220m')

training_set = OpcodeData(X_train, token, max_length)
validating_set = OpcodeData(X_val, token, max_length)
testing_set = OpcodeData(X_test, token, max_length)
training_loader = DataLoader(training_set, batch_size=4,shuffle=True)
validating_loader = DataLoader(validating_set, batch_size=4,shuffle=True)
testing_loader = DataLoader(testing_set, batch_size=4,shuffle=True)

criterion = nn.CrossEntropyLoss()


path ='/home/bkcs/HDD/Thep/result/nonblock/3'

def evaluate_steps_test(model,validate_loader):
    print("Evaluating...")
    model.eval()

    total_loss = 0
    total_preds = torch.tensor([])  # Initialize as empty tensor
    total_labels = torch.tensor([])


    for  i, data  in enumerate(validate_loader):
        # push the batch to gpu
        inputs, labels = data['inputs'].to(model.local_rank), data['targets'].to(
            model.local_rank)


        with torch.no_grad():
            if fp16:
                inputs = inputs.half()
            outputs = model(inputs)
 
            if i == 0:  # Initialize tensors only in the first iteration
                total_preds = torch.empty(0, *outputs.shape[1:], device=outputs.device)
                total_labels = torch.empty(0, *labels.shape[1:], device=labels.device)
            total_preds = torch.cat((total_preds, (outputs>0.5)), dim=0)
            total_labels = torch.cat((total_labels, labels), dim=0)



        
    TP = ((total_preds > 0.5) & (total_labels > 0.5)).sum(1).float() 
    TN = ((total_preds <= 0.5) & (total_labels <= 0.5)).sum(1).float() 
    FP = ((total_preds > 0.5) & (total_labels <= 0.5)).sum(1).float() 
    FN = ((total_preds <= 0.5) & (total_labels > 0.5)).sum(1).float() 

    precision = torch.mean(TP / (TP + FP + 1e-12))
    recall = torch.mean(TP / (TP + FN + 1e-12))
    f2 =  precision * recall / (precision + recall + 1e-12)
    labels = total_labels.cpu().numpy()
    preds = total_preds.cpu().numpy()

    label_names = ['Delegatecall Injection', 'Frozen Ether', 'Outdated Solidity version', 'Reentrancy', 'Timestamp dependence']
    out = classification_report( labels,preds, output_dict=True,target_names=label_names)
    total_support = i
    acc = accuracy_score(labels,preds)
    hm = hamming_loss(labels,preds)

    acc_s =  torch.mean(TP/(TP+FN+FP)).cpu().numpy()

    out['Exact Match Ratio'] = {'precision': acc, 'recall': acc, 'f1-score': acc, 'support': total_support}
    out['Hamming Loss'] = {'precision': hm, 'recall': hm, 'f1-score': hm, 'support': total_support}
    out['Accuracy'] = {'precision': acc_s, 'recall': acc_s, 'f1-score': acc_s, 'support': total_support}
    out_df = pd.DataFrame(out).transpose()
    print(out_df)

    out_df.to_csv(path+"result.csv")
    # print(i)
    return out_df 

with torch.no_grad():
    evaluate_steps_test(model_engine, testing_loader)
            




print('Finished Training')



    

