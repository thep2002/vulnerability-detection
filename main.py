import argparse
import json
import logging
import os
from time import time

import dgl
import numpy as np
import torch
import time
import torch.nn 
import torch.nn.functional as F
from sklearn.metrics import classification_report,accuracy_score,hamming_loss
from dgl.data import DGLDataset
from dgl.dataloading import GraphDataLoader
from networks import HGPSLModel
from torch.utils.data import random_split
from utils import get_stats
import pandas as pd

class SyntheticDataset(DGLDataset):
    def __init__(self):
        super().__init__(name="synthetic")

    def process(self):
        dataset = torch.load('/home/bkcs/HDD/data_graph/batch_dgl_graph_tune.pt')
        self.graphs = dgl.unbatch(dataset)
        self.labels = pd.read_csv('/home/bkcs/HDD/data_graph/label_tune.csv')
        self.labels = self.labels.drop(self.labels.columns[0], axis=1)
        labels_numpy = self.labels.values
        self.labels = torch.from_numpy(labels_numpy)

    def __getitem__(self, i):
        return self.graphs[i], self.labels[i]
    def seti(self,i):
        self.graphs[i] = dgl.add_self_loop(self.graphs[i])

    def __len__(self):
        return len(self.graphs)
def parse_args():
    parser = argparse.ArgumentParser(description="HGP-SL-DGL")
    parser.add_argument(
        "--dataset",
        type=str,
        default="DD",
        choices=["DD", "PROTEINS", "NCI1", "NCI109", "Mutagenicity", "ENZYMES"],
        help="DD/PROTEINS/NCI1/NCI109/Mutagenicity/ENZYMES",
    )
    parser.add_argument(
        "--batch_size", type=int, default=512, help="batch size"
    )
    parser.add_argument(
        "--sample", type=str, default="true", help="use sample method"
    )
    parser.add_argument("--lr", type=float, default=1e-3, help="learning rate")
    parser.add_argument(
        "--weight_decay", type=float, default=1e-4, help="weight decay"
    )
    parser.add_argument(
        "--pool_ratio", type=float, default=0.5, help="pooling ratio"
    )
    parser.add_argument("--hid_dim", type=int, default=1024, help="hidden size")
    parser.add_argument(
        "--conv_layers", type=int, default=3, help="number of conv layers"
    )
    parser.add_argument(
        "--dropout", type=float, default=0.0, help="dropout ratio"
    )
    parser.add_argument(
        "--lamb", type=float, default=1.25, help="trade-off parameter"
    )
    parser.add_argument(
        "--epochs", type=int, default=1000, help="max number of training epochs"
    )
    parser.add_argument(
        "--patience", type=int, default=100, help="patience for early stopping"
    )
    parser.add_argument(
        "--device", type=int, default=-1, help="device id, -1 for cpu"
    )
    parser.add_argument(
        "--dataset_path", type=str, default="./dataset", help="path to dataset"
    )
    parser.add_argument(
        "--print_every",
        type=int,
        default=10,
        help="print trainlog every k epochs, -1 for silent training",
    )
    parser.add_argument(
        "--num_trials", type=int, default=1, help="number of trials"
    )
    parser.add_argument("--output_path", type=str, default="./output")

    args = parser.parse_args()

    # device
    args.device = "cpu" if args.device == -1 else "cuda:{}".format(args.device)
    if not torch.cuda.is_available():
        logging.warning("CUDA is not available, use CPU for training.")
        args.device = "cpu"

    # print every
    if args.print_every == -1:
        args.print_every = args.epochs + 1

    # bool args
    if args.sample.lower() == "true":
        args.sample = True
    else:
        args.sample = False

    # paths
    if not os.path.exists(args.dataset_path):
        os.makedirs(args.dataset_path)
    if not os.path.exists(args.output_path):
        os.makedirs(args.output_path)
    name = (
        "Data={}_Hidden={}_Pool={}_WeightDecay={}_Lr={}_Sample={}.log".format(
            args.dataset,
            args.hid_dim,
            args.pool_ratio,
            args.weight_decay,
            args.lr,
            args.sample,
        )
    )
    args.output_path = os.path.join(args.output_path, name)

    return args


def train(model: torch.nn.Module, optimizer, trainloader, device,bce_loss):
    model.train()
    total_loss = 0.0
    num_batches = len(trainloader)
    for batch in trainloader:

        optimizer.zero_grad()
        batch_graphs,batch_labels = batch
        batch_graphs = batch_graphs.to(device)

        batch_labels = batch_labels.float().to(device)
        out = model(batch_graphs, batch_graphs.ndata["feat"])
        loss = bce_loss(out,batch_labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / num_batches


@torch.no_grad()
def test(model: torch.nn.Module, loader, device,bce_loss):
    model.eval()
    sample_runtimes = []
    correct = 0.0
    loss = 0.0
    num_graphs = 0
    for i,batch in enumerate(loader):
        batch_graphs, batch_labels = batch
        num_graphs += batch_labels.size(0)
        batch_graphs = batch_graphs.to(device)
        batch_labels = batch_labels.float().to(device)
        start_time_k = time.time()
        outputs = model(batch_graphs, batch_graphs.ndata["feat"])
        end_time_k = time.time()
        if i == 0:  # Initialize tensors only in the first iteration
            total_preds = torch.empty(0, *outputs.shape[1:], device=outputs.device)
            total_labels = torch.empty(0, *batch_labels.shape[1:], device=batch_labels.device)
        total_preds = torch.cat((total_preds, (outputs>0.5)), dim=0)
        total_labels = torch.cat((total_labels, batch_labels), dim=0)
        loss += bce_loss(outputs, batch_labels).item()
        sample_runtimes.append(end_time_k - start_time_k)
    average_runtime = np.mean(sample_runtimes)/i
    TP = ((total_preds > 0.5) & (total_labels > 0.5)).sum(1).float() 
    TN = ((total_preds <= 0.5) & (total_labels <= 0.5)).sum(1).float() 
    FP = ((total_preds > 0.5) & (total_labels <= 0.5)).sum(1).float() 
    FN = ((total_preds <= 0.5) & (total_labels > 0.5)).sum(1).float() 
    acc_s =  torch.mean(TP/(TP+FN+FP)).cpu().numpy()
    labels = total_labels.cpu().numpy()
    preds = total_preds.cpu().numpy()

    label_names = [ 'access-control', 'arithmetic', 'reentrancy',  'unchecked-calls']
    out = classification_report( labels,preds, output_dict=True,target_names=label_names)
    total_support = out['samples avg']['support']
    acc = accuracy_score(labels,preds)
    hm = hamming_loss(labels,preds)

    acc_s =  torch.mean(TP/(TP+FN+FP)).cpu().numpy()

    out['Exact Match Ratio'] = {'precision': acc, 'recall': acc, 'f1-score': acc, 'support': total_support}
    out['Hamming Loss'] = {'precision': hm, 'recall': hm, 'f1-score': hm, 'support': total_support}
    out['Accuracy'] = {'precision': acc_s, 'recall': acc_s, 'f1-score': acc_s, 'support': total_support}
    out['Timepersample'] = {'precision': average_runtime, 'recall': average_runtime, 'f1-score': average_runtime, 'support': average_runtime}
    
    out_df = pd.DataFrame(out).transpose()
    return acc_s, loss / num_graphs,out_df


def main(args):
    dataset = SyntheticDataset()
    dataset.process()
    for i in range(len(dataset)):
        dataset.seti(i)
    num_training = int(len(dataset) * 0.85)
    num_val = int(len(dataset) * 0.05)
    num_test = len(dataset) - num_val - num_training
    train_set, val_set, test_set = random_split(
        dataset, [num_training, num_val, num_test]
    )

    train_loader = GraphDataLoader(
        train_set, batch_size=args.batch_size, shuffle=True, num_workers=6
    )
    val_loader = GraphDataLoader(
        val_set, batch_size=args.batch_size, num_workers=2
    )
    test_loader = GraphDataLoader(
        test_set, batch_size=args.batch_size, num_workers=2
    )

    device = torch.device(args.device)

    # Step 2: Create model =================================================================== #
    num_feature, num_classes= 768,4

    model = HGPSLModel(
        in_feat=num_feature,
        out_feat=num_classes,
        hid_feat=args.hid_dim,
        conv_layers=args.conv_layers,
        dropout=args.dropout,
        pool_ratio=args.pool_ratio,
        lamb=args.lamb,
        sample=args.sample,
    ).to(device)
    args.num_feature = int(num_feature)
    args.num_classes = int(num_classes)
    bce_loss = torch.nn.BCELoss()
    # Step 3: Create training components ===================================================== #
    optimizer = torch.optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    # Step 4: training epoches =============================================================== #
    bad_cound = 0
    best_val_acc = 0
    final_test_acc = 0.0
    best_epoch = 0
    train_times = []
    for e in range(args.epochs):
        s_time = time.time()
        train_loss = train(model, optimizer, train_loader, device,bce_loss)
        train_times.append(time.time() - s_time)
        val_acc, val_loss,_ = test(model, val_loader, device,bce_loss)
        test_acc, _,out = test(model, test_loader, device,bce_loss)
        if best_val_acc < val_acc:
            best_val_acc = val_acc
            final_test_acc = test_acc
            bad_cound = 0
            out.to_csv("result_tune.csv")
            best_epoch = e + 1
            
        else:
            bad_cound += 1
        # if bad_cound >= args.patience:
        #     break

        if (e + 1) % args.print_every == 0:
            log_format = (
                "Epoch {}: loss={:.4f}, val_acc={:.4f}, final_test_acc={:.4f}"
            )
            print(log_format.format(e + 1, train_loss, val_acc, final_test_acc))
    print(
        "Best Epoch {}, final test acc {:.4f}".format(
            best_epoch, final_test_acc
        )
    )
    return final_test_acc, sum(train_times) / len(train_times)


if __name__ == "__main__":
    args = parse_args()
    print(args)
    res = []
    train_times = []
    for i in range(args.num_trials):
        print("Trial {}/{}".format(i + 1, args.num_trials))
        acc, train_time = main(args)
        res.append(acc)
        train_times.append(train_time)

    # mean, err_bd = get_stats(res, conf_interval=False)
    # print("mean acc: {:.4f}, error bound: {:.4f}".format(mean, err_bd))

    # out_dict = {
    #     "hyper-parameters": vars(args),
    #     "result": "{:.4f}(+-{:.4f})".format(mean, err_bd),
    #     "train_time": "{:.4f}".format(sum(train_times) / len(train_times)),
    # }

    # with open(args.output_path, "w") as f:
    #     json.dump(out_dict, f, sort_keys=True, indent=4)
